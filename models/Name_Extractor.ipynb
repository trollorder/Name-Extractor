{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoknqaGmUDJ0",
        "outputId": "81a72aeb-762f-4eab-f8d3-7d571a444f91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Using cached transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
            "Requirement already satisfied: filelock in c:\\python311\\lib\\site-packages (from transformers) (3.12.3)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\python311\\lib\\site-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python311\\lib\\site-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\python311\\lib\\site-packages (from transformers) (2023.8.8)\n",
            "Collecting requests (from transformers)\n",
            "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\python311\\lib\\site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\python311\\lib\\site-packages (from transformers) (0.3.3)\n",
            "Collecting tqdm>=4.27 (from transformers)\n",
            "  Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "Requirement already satisfied: fsspec in c:\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
            "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
            "  Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
            "Installing collected packages: tqdm, certifi, requests, huggingface-hub, transformers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
            "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'C:\\\\Python311\\\\Scripts\\\\tqdm.exe' -> 'C:\\\\Python311\\\\Scripts\\\\tqdm.exe.deleteme'\n",
            "\n",
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
            "[notice] To update, run: C:\\Python311\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\python311\\lib\\site-packages (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in c:\\python311\\lib\\site-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in c:\\python311\\lib\\site-packages (from scikit-learn) (1.11.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\python311\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python311\\lib\\site-packages (from scikit-learn) (3.2.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
            "[notice] To update, run: C:\\Python311\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIL_UxMZUhSD"
      },
      "source": [
        "#Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgHe10fnWYQF",
        "outputId": "2d760089-ec1e-4110-d2bd-ea1ea1260d8f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "excel_name = \"Cleansed OT.agents.xlsx\"\n",
        "dataset = pd.read_excel(excel_name, sheet_name=\"Sheet1\")\n",
        "\n",
        "def generate_start_idx_col(dataset, label, context):\n",
        "    for index, row in dataset.iterrows():\n",
        "        context_col = context\n",
        "        find_col = label\n",
        "        start_idx = row[context_col].find(row[find_col])\n",
        "        if start_idx <0:\n",
        "          start_idx = 0\n",
        "        end_idx = start_idx + len(row[find_col]) - 1\n",
        "        dataset.at[index, \"start_idx\"] = int(start_idx)\n",
        "        dataset.at[index, \"end_idx\"] = int(end_idx)\n",
        "\n",
        "\n",
        "#get idx start and end\n",
        "generate_start_idx_col(dataset, \"name\", \"full name\")\n",
        "#add same question\n",
        "question_prompt = \"How do I address him?\"\n",
        "dataset[\"question\"] = question_prompt\n",
        "dataset.to_csv(\"check.csv\")\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "train_df, test_df = train_test_split(dataset, test_size=0.1, random_state=42)\n",
        "test_df, val_df = train_test_split(test_df, test_size=0.1, random_state=42)\n",
        "train_contexts, train_questions, train_answers = train_df[\"full name\"], train_df[\"question\"], train_df[[\"name\", \"start_idx\", \"end_idx\"]]\n",
        "val_contexts, val_questions, val_answers = val_df[\"full name\"], val_df[\"question\"], val_df[[\"name\", \"start_idx\", \"end_idx\"]]\n",
        "test_contexts, test_questions, test_answers = test_df[\"full name\"], test_df[\"question\"], test_df[[\"name\", \"start_idx\", \"end_idx\"]]\n",
        "\n",
        "# Printing the size of each set to verify\n",
        "print(f\"Train set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")\n",
        "\n",
        "\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
        "\n",
        "model_name = \"deepset/roberta-base-squad2\"\n",
        "\n",
        "# b) Load model & tokenizer\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "#prep format to list\n",
        "# Tokenize contexts and questions separately\n",
        "train_encodings = tokenizer(list(train_contexts), list(train_questions), truncation=True, padding=True)\n",
        "val_encodings = tokenizer(list(val_contexts), list(val_questions), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(list(test_contexts) , list(test_questions) , truncation=True, padding=True )\n",
        "\n",
        "print(train_answers)\n",
        "def add_token_positions(encodings, answers):\n",
        "    # initialize lists to contain the token indices of answer start/end\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i , tuple_row in enumerate(answers.iterrows()) :\n",
        "        # append start/end token position using char_to_token method\n",
        "        start_positions.append(encodings.char_to_token(i,int(tuple_row[1][\"start_idx\"])))\n",
        "        end_positions.append(encodings.char_to_token(i,int(tuple_row[1][\"end_idx\"])))\n",
        "\n",
        "        # if start position is None, the answer passage has been truncated\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "        # end position cannot be found, char_to_token found space, so shift one token forward\n",
        "        go_back = 1\n",
        "        while end_positions[-1] is None:\n",
        "            end_positions[-1] = encodings.char_to_token(i, int(tuple_row[1][\"end_idx\"]-go_back))\n",
        "            go_back +=1\n",
        "    # update our encodings object with the new token-based start/end positions\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "# Apply function to our data\n",
        "add_token_positions(train_encodings, train_answers)\n",
        "add_token_positions(val_encodings, val_answers)\n",
        "add_token_positions(test_encodings, test_answers)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNVleIxSmhOJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset = SquadDataset(train_encodings)\n",
        "val_dataset = SquadDataset(val_encodings)\n",
        "test_dataset= SquadDataset(test_encodings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcGfprXKmob1",
        "outputId": "aa476db1-2c2d-4942-d92f-d0500ccc9f70"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 21/21 [00:12<00:00,  1.68it/s, loss=0.00353]\n",
            "Epoch 1: 100%|██████████| 21/21 [00:12<00:00,  1.69it/s, loss=0.0122]\n",
            "Epoch 2: 100%|██████████| 21/21 [00:12<00:00,  1.67it/s, loss=0.00629]\n",
            "Epoch 3: 100%|██████████| 21/21 [00:12<00:00,  1.66it/s, loss=0.00308]\n",
            "Epoch 4: 100%|██████████| 21/21 [00:12<00:00,  1.66it/s, loss=0.00618]\n",
            "Epoch 5: 100%|██████████| 21/21 [00:12<00:00,  1.65it/s, loss=0.00648]\n",
            "Epoch 6: 100%|██████████| 21/21 [00:12<00:00,  1.64it/s, loss=0.00144]\n",
            "Epoch 7: 100%|██████████| 21/21 [00:12<00:00,  1.63it/s, loss=0.0102]\n",
            "Epoch 8: 100%|██████████| 21/21 [00:12<00:00,  1.63it/s, loss=0.00553]\n",
            "Epoch 9: 100%|██████████| 21/21 [00:12<00:00,  1.63it/s, loss=0.00184]\n",
            "Epoch 10: 100%|██████████| 21/21 [00:12<00:00,  1.62it/s, loss=0.0103]\n",
            "Epoch 11: 100%|██████████| 21/21 [00:13<00:00,  1.59it/s, loss=0.00215]\n",
            "Epoch 12: 100%|██████████| 21/21 [00:13<00:00,  1.54it/s, loss=0.00818]\n",
            "Epoch 13: 100%|██████████| 21/21 [00:13<00:00,  1.51it/s, loss=0.00124]\n",
            "Epoch 14: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.0106]\n",
            "Epoch 15: 100%|██████████| 21/21 [00:13<00:00,  1.59it/s, loss=0.000743]\n",
            "Epoch 16: 100%|██████████| 21/21 [00:13<00:00,  1.59it/s, loss=0.00403]\n",
            "Epoch 17: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00215]\n",
            "Epoch 18: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.00302]\n",
            "Epoch 19: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.0203]\n",
            "Epoch 20: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000857]\n",
            "Epoch 21: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00281]\n",
            "Epoch 22: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.0042]\n",
            "Epoch 23: 100%|██████████| 21/21 [00:13<00:00,  1.58it/s, loss=0.00487]\n",
            "Epoch 24: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.0231]\n",
            "Epoch 25: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000643]\n",
            "Epoch 26: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00194]\n",
            "Epoch 27: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000668]\n",
            "Epoch 28: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00114]\n",
            "Epoch 29: 100%|██████████| 21/21 [00:13<00:00,  1.53it/s, loss=0.00105]\n",
            "Epoch 30: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00155]\n",
            "Epoch 31: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.00448]\n",
            "Epoch 32: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.00764]\n",
            "Epoch 33: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.0141]\n",
            "Epoch 34: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.0022]\n",
            "Epoch 35: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.00126]\n",
            "Epoch 36: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000811]\n",
            "Epoch 37: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000237]\n",
            "Epoch 38: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.000386]\n",
            "Epoch 39: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00128]\n",
            "Epoch 40: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00186]\n",
            "Epoch 41: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.0014]\n",
            "Epoch 42: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000568]\n",
            "Epoch 43: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00089]\n",
            "Epoch 44: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000669]\n",
            "Epoch 45: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000407]\n",
            "Epoch 46: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00292]\n",
            "Epoch 47: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.00253]\n",
            "Epoch 48: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.0018]\n",
            "Epoch 49: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00115]\n",
            "Epoch 50: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00104]\n",
            "Epoch 51: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000583]\n",
            "Epoch 52: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00282]\n",
            "Epoch 53: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000279]\n",
            "Epoch 54: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.000455]\n",
            "Epoch 55: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00192]\n",
            "Epoch 56: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00082]\n",
            "Epoch 57: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000752]\n",
            "Epoch 58: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.00601]\n",
            "Epoch 59: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.00174]\n",
            "Epoch 60: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000456]\n",
            "Epoch 61: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000373]\n",
            "Epoch 62: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.0808]\n",
            "Epoch 63: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.000358]\n",
            "Epoch 64: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000213]\n",
            "Epoch 65: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00199]\n",
            "Epoch 66: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00152]\n",
            "Epoch 67: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000515]\n",
            "Epoch 68: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000695]\n",
            "Epoch 69: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000664]\n",
            "Epoch 70: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00777]\n",
            "Epoch 71: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00059]\n",
            "Epoch 72: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00203]\n",
            "Epoch 73: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000441]\n",
            "Epoch 74: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.0222]\n",
            "Epoch 75: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.000372]\n",
            "Epoch 76: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.00119]\n",
            "Epoch 77: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.000911]\n",
            "Epoch 78: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.0133]\n",
            "Epoch 79: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000466]\n",
            "Epoch 80: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000923]\n",
            "Epoch 81: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.0161]\n",
            "Epoch 82: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000351]\n",
            "Epoch 83: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00219]\n",
            "Epoch 84: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00949]\n",
            "Epoch 85: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000313]\n",
            "Epoch 86: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00234]\n",
            "Epoch 87: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00518]\n",
            "Epoch 88: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.000201]\n",
            "Epoch 89: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.0185]\n",
            "Epoch 90: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000555]\n",
            "Epoch 91: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000437]\n",
            "Epoch 92: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00174]\n",
            "Epoch 93: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.000849]\n",
            "Epoch 94: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.0131]\n",
            "Epoch 95: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.000874]\n",
            "Epoch 96: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.000569]\n",
            "Epoch 97: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.000774]\n",
            "Epoch 98: 100%|██████████| 21/21 [00:13<00:00,  1.56it/s, loss=0.000653]\n",
            "Epoch 99: 100%|██████████| 21/21 [00:13<00:00,  1.57it/s, loss=0.00106]\n",
            "100%|██████████| 2/2 [00:00<00:00, 27.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "T/F\tstart\tend\n",
            "\n",
            "true\t2\t6\n",
            "pred\t2\t6\n",
            "\n",
            "true\t9\t11\n",
            "pred\t9\t11\n",
            "\n",
            "true\t8\t9\n",
            "pred\t8\t9\n",
            "\n",
            "true\t8\t9\n",
            "pred\t8\t9\n",
            "\n",
            "true\t3\t6\n",
            "pred\t3\t6\n",
            "\n",
            "true\t8\t8\n",
            "pred\t8\t8\n",
            "\n",
            "true\t11\t12\n",
            "pred\t11\t12\n",
            "\n",
            "true\t3\t7\n",
            "pred\t3\t7\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "\n",
        "# setup GPU/CPU\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# move model over to detected device\n",
        "model.to(device)\n",
        "# activate training mode of model\n",
        "model.train()\n",
        "# initialize adam optimizer with weight decay (reduces chance of overfitting)\n",
        "optim = AdamW(model.parameters(), lr=1e-6)\n",
        "\n",
        "# initialize data loader for training data\n",
        "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
        "number_of_epochs = 100\n",
        "scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=0, num_training_steps=len(train_loader)*number_of_epochs)\n",
        "\n",
        "\n",
        "for epoch in range(number_of_epochs):\n",
        "    # set model to train mode\n",
        "    model.train()\n",
        "    # setup loop (we use tqdm for the progress bar)\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    for batch in loop:\n",
        "        # initialize calculated gradients (from prev step)\n",
        "        optim.zero_grad()\n",
        "        # pull all the tensor batches required for training\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "        # train model on batch and return outputs (incl. loss)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask,\n",
        "                        start_positions=start_positions,\n",
        "                        end_positions=end_positions)\n",
        "        # extract loss\n",
        "        loss = outputs[0]\n",
        "        # calculate loss for every parameter that needs grad update\n",
        "        loss.backward()\n",
        "        # update parameters\n",
        "        optim.step()\n",
        "        # print relevant info to progress bar\n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "    scheduler.step()\n",
        "\n",
        "# switch model out of training mode\n",
        "model_path = 'name_extractor_model'\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "model.eval()\n",
        "\n",
        "#val_sampler = SequentialSampler(val_dataset)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "\n",
        "acc = []\n",
        "\n",
        "# initialize loop for progress bar\n",
        "loop = tqdm(val_loader)\n",
        "# loop through batches\n",
        "for batch in loop:\n",
        "    # we don't need to calculate gradients as we're not training\n",
        "    with torch.no_grad():\n",
        "        # pull batched items from loader\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_true = batch['start_positions'].to(device)\n",
        "        end_true = batch['end_positions'].to(device)\n",
        "        # make predictions\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        # pull preds out\n",
        "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
        "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
        "        # calculate accuracy for both and append to accuracy list\n",
        "        acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n",
        "        acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n",
        "# calculate average accuracy in total\n",
        "acc = sum(acc)/len(acc)\n",
        "print(\"T/F\\tstart\\tend\\n\")\n",
        "for i in range(len(start_true)):\n",
        "    print(f\"true\\t{start_true[i]}\\t{end_true[i]}\\n\"\n",
        "          f\"pred\\t{start_pred[i]}\\t{end_pred[i]}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPgxg048T7tm",
        "outputId": "5f558c04-4eb6-4e2a-f398-254911afd2ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/name_extractor_model/ (stored 0%)\n",
            "  adding: content/name_extractor_model/tokenizer.json (deflated 72%)\n",
            "  adding: content/name_extractor_model/merges.txt (deflated 53%)\n",
            "  adding: content/name_extractor_model/tokenizer_config.json (deflated 80%)\n",
            "  adding: content/name_extractor_model/pytorch_model.bin (deflated 8%)\n",
            "  adding: content/name_extractor_model/special_tokens_map.json (deflated 85%)\n",
            "  adding: content/name_extractor_model/config.json (deflated 49%)\n",
            "  adding: content/name_extractor_model/vocab.json (deflated 59%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r /content/model.zip /content/name_extractor_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoWJ3n1PUlgE"
      },
      "source": [
        "#Download and Store Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "dJ7vVeiTVfHC",
        "outputId": "2d8b9ebb-8c7b-4263-d987-69647df14c7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/model.zip'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Copy the file to Google Drive\n",
        "shutil.copy(\"/content/model.zip\", \"/content/drive/MyDrive/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpXyu7mOUqXO"
      },
      "source": [
        "#Load and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTtbI7DA7WZc",
        "outputId": "fa1891ee-828a-4b34-bcff-6d7441fe369d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Loaded\n",
            "Processing 2301 entries\n",
            "Progress is at 0%\n",
            "Progress is at 1%\n",
            "Progress is at 2%\n",
            "Progress is at 3%\n",
            "Progress is at 4%\n",
            "Progress is at 5%\n",
            "Progress is at 6%\n",
            "Progress is at 7%\n",
            "Progress is at 8%\n",
            "Progress is at 9%\n",
            "Progress is at 10%\n",
            "Progress is at 11%\n",
            "Progress is at 12%\n",
            "Progress is at 13%\n",
            "Progress is at 14%\n",
            "Progress is at 15%\n",
            "Progress is at 16%\n",
            "Progress is at 17%\n",
            "Progress is at 18%\n",
            "Progress is at 19%\n",
            "Progress is at 20%\n",
            "Progress is at 21%\n",
            "Progress is at 22%\n",
            "Progress is at 23%\n",
            "Progress is at 24%\n",
            "Progress is at 25%\n",
            "Progress is at 26%\n",
            "Progress is at 27%\n",
            "Progress is at 28%\n",
            "Progress is at 29%\n",
            "Progress is at 30%\n",
            "Progress is at 31%\n",
            "Progress is at 32%\n",
            "Progress is at 33%\n",
            "Progress is at 34%\n",
            "Progress is at 35%\n",
            "Progress is at 36%\n",
            "Progress is at 37%\n",
            "Progress is at 38%\n",
            "Progress is at 39%\n",
            "Progress is at 40%\n",
            "Progress is at 41%\n",
            "Progress is at 42%\n",
            "Progress is at 43%\n",
            "Progress is at 44%\n",
            "Progress is at 45%\n",
            "Progress is at 46%\n",
            "Progress is at 47%\n",
            "Progress is at 48%\n",
            "Progress is at 49%\n",
            "Progress is at 50%\n",
            "Progress is at 51%\n",
            "Progress is at 52%\n",
            "Progress is at 53%\n",
            "Progress is at 54%\n",
            "Progress is at 55%\n",
            "Progress is at 56%\n",
            "Progress is at 57%\n",
            "Progress is at 58%\n",
            "Progress is at 59%\n",
            "Progress is at 60%\n",
            "Progress is at 61%\n",
            "Progress is at 62%\n",
            "Progress is at 63%\n",
            "Progress is at 64%\n",
            "Progress is at 65%\n",
            "Progress is at 66%\n",
            "Progress is at 67%\n",
            "Progress is at 68%\n",
            "Progress is at 69%\n",
            "Progress is at 70%\n",
            "Progress is at 71%\n",
            "Progress is at 72%\n",
            "Progress is at 73%\n",
            "Progress is at 74%\n",
            "Progress is at 75%\n",
            "Progress is at 76%\n",
            "Progress is at 77%\n",
            "Progress is at 78%\n",
            "Progress is at 79%\n",
            "Progress is at 80%\n",
            "Progress is at 81%\n",
            "Progress is at 82%\n",
            "Progress is at 83%\n",
            "Progress is at 84%\n",
            "Progress is at 85%\n",
            "Progress is at 86%\n",
            "Progress is at 87%\n",
            "Progress is at 88%\n",
            "Progress is at 89%\n",
            "Progress is at 90%\n",
            "Progress is at 91%\n",
            "Progress is at 92%\n",
            "Progress is at 93%\n",
            "Progress is at 94%\n",
            "Progress is at 95%\n",
            "Progress is at 96%\n",
            "Progress is at 97%\n",
            "Progress is at 98%\n",
            "Progress is at 99%\n",
            "Processing done, saving to excel\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
        "import pandas as pd\n",
        "\n",
        "# Path to the directory where you saved the model and tokenizer\n",
        "model_path = 'name_extractor_model'\n",
        "\n",
        "# Load the pretrained model and tokenizer\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize empty lists for storing predictions and ground truths\n",
        "start_pred_list = []\n",
        "end_pred_list = []\n",
        "start_true_list = []\n",
        "end_true_list = []\n",
        "predicted_answers = []\n",
        "\n",
        "model.eval()\n",
        "print(\"Model Loaded\")\n",
        "\n",
        "# Print the predicted answers along with the true labels\n",
        "predicted_answers = []\n",
        "input_contexts= list(pd.read_excel(\"testing.xlsx\", sheet_name = \"raw\").loc[:,\"full name\"])\n",
        "questions = [\"How do I address him?\" for x in range(len(input_contexts))]\n",
        "number_of_entries = len(input_contexts)\n",
        "# Iterate through input contexts and questions\n",
        "print(f\"Processing {number_of_entries} entries\")\n",
        "\n",
        "# Progress\n",
        "interval = int(number_of_entries/100)\n",
        "\n",
        "\n",
        "for index,(context, question) in enumerate(zip(input_contexts, questions)):\n",
        "    # Encode the input context and question\n",
        "    inputs = tokenizer(context, question, return_tensors='pt', truncation=True, padding=True)\n",
        "    inputs.to(device)\n",
        "\n",
        "    # Make predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits = outputs.end_logits\n",
        "\n",
        "    # Get the predicted answer span\n",
        "    start_pos = torch.argmax(start_logits)\n",
        "    end_pos = torch.argmax(end_logits) + 1  # Adding 1 to include the end position\n",
        "\n",
        "    # Decode the predicted answer span using the tokenizer\n",
        "    predicted_answer = tokenizer.decode(inputs['input_ids'][0][start_pos:end_pos], skip_special_tokens=True)\n",
        "    predicted_answer = predicted_answer.title().strip()\n",
        "    predicted_answers.append(predicted_answer)\n",
        "    if index %interval == 0 and index!= 0:\n",
        "        print(f\"Progress is at {int(index/number_of_entries*100)}%\")\n",
        "\n",
        "print(\"Processing done, saving to excel\")\n",
        "temp_df = pd.DataFrame(zip(input_contexts,predicted_answers) , columns=[\"Full Name\" , \"Name\"])\n",
        "temp_df.to_excel(\"predicted.xlsx\" , index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3WWL44GMHDe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
